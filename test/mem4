import AVFoundation

func extractAudioFromMP4(inputFilePath: String, outputFilePath: String) -> Bool {
    let asset = AVAsset(url: URL(fileURLWithPath: inputFilePath))
    
    guard let audioTrack = asset.tracks(withMediaType: .audio).first else {
        print("未找到音频轨道")
        return false
    }
    
    let outputSettings: [String: Any] = [
        AVFormatIDKey: kAudioFormatLinearPCM,
        AVLinearPCMIsFloatKey: false,
        AVLinearPCMIsBigEndianKey: false,
        AVLinearPCMBitDepthKey: 16,
        AVLinearPCMIsNonInterleaved: false
    ]
    
    let outputURL = URL(fileURLWithPath: outputFilePath)
    
    do {
        let assetReader = try AVAssetReader(asset: asset)
        let trackOutput = AVAssetReaderTrackOutput(track: audioTrack, outputSettings: outputSettings)
        
        assetReader.add(trackOutput)
        
        assetReader.startReading()
        
        let fileOutput = FileHandle(forWritingAtPath: outputFilePath)
        fileOutput?.truncateFile(atOffset: 0)
        
        let audioFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: audioTrack.nominalFrameRate, channels: audioTrack.formatDescriptions.first.map { $0.channelCount } ?? 0, interleaved: false)
        
        let audioFile = try AVAudioFile(forWriting: outputURL, settings: audioFormat!.settings, commonFormat: .pcmFormatInt16, interleaved: false)
        
        while let sampleBuffer = trackOutput.copyNextSampleBuffer() {
            let presentationTimeStamp = CMSampleBufferGetOutputPresentationTimeStamp(sampleBuffer)
            let audioBuffer = CMSampleBufferGetDataBuffer(sampleBuffer)
            
            var blockBuffer: CMBlockBuffer?
            var audioBufferList = AudioBufferList(
                mNumberBuffers: 1,
                mBuffers: AudioBuffer(
                    mNumberChannels: UInt32(audioTrack.formatDescriptions.first.map { $0.channelCount } ?? 0),
                    mDataByteSize: CMBlockBufferGetDataLength(audioBuffer!),
                    mData: nil
                )
            )
            
            let status = CMSampleBufferGetAudioBufferListWithRetainedBlockBuffer(
                sampleBuffer,
                bufferListSizeNeededOut: nil,
                bufferListOut: &audioBufferList,
                bufferListSize: MemoryLayout<AudioBufferList>.size,
                blockBufferAllocator: nil,
                blockBufferMemoryAllocator: nil,
                flags: kCMSampleBufferFlag_AudioBufferList_Assure16ByteAlignment,
                blockBufferOut: &blockBuffer
            )
            
            guard status == 0 else {
                print("无法获取 PCM 缓冲区")
                return false
            }
            
            let audioBufferData = Data(bytes: audioBufferList.mBuffers.mData!, count: Int(audioBufferList.mBuffers.mDataByteSize))
            
            let audioBufferPCM = AVAudioPCMBuffer(pcmFormat: audioFormat!, frameCapacity: AVAudioFrameCount(audioTrack.formatDescriptions.first.map { $0.channelCount } ?? 0))
            audioBufferPCM?.frameLength = AVAudioFrameCount(audioBufferData.count) / audioFormat!.streamDescription.pointee.mBytesPerFrame
            memcpy(audioBufferPCM!.floatChannelData![0], audioBufferData.bytes, audioBufferData.count)
            
            try audioFile.write(from: audioBufferPCM!)
            
            CMSampleBufferInvalidate(sampleBuffer)
            CFRelease(sampleBuffer)
            CFRelease(blockBuffer)
        }
        
        assetReader.cancelReading()
        fileOutput?.closeFile()
        
        print("音频已解码并保存为 WAV 文件：\(outputFilePath)")
        return true
    } catch {
        print("音频解码失败：\(error)")
        return false
    }
}

// 使用示例
let inputFilePath = "path/to/inputfile.mp4"
let outputFilePath = "path/to/outputfile.wav"

let extractionSuccess = extractAudioFromMP4(inputFilePath: inputFilePath, outputFilePath: outputFilePath)
if extractionSuccess {
    print("音频提取和解码成功")
} else {
    print("音频提取和解码失败")
}
